{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time, os\n",
    "import pandas as pd, numpy as np, tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "import category_encoders as ce\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import pandas_profiling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "rcParams.update({'figure.autolayout': True, 'axes.titlepad': 20})\n",
    "rcParams['figure.figsize'] = 20,8\n",
    "import h5py\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - pre-process raw data data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_unicode(filename):\n",
    "    import codecs\n",
    "    f1 = open(filename.split('.csv')[0] + '_corrected.csv', \"w\")\n",
    "    with codecs.open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for l in f.readlines()[:170367]:\n",
    "            f1.write(l)\n",
    "    f1.close()\n",
    "    \n",
    "correct_unicode(\"data/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_files = [\n",
    "        'data/Monday-WorkingHours.pcap_ISCX.csv',\n",
    "        'data/Tuesday-WorkingHours.pcap_ISCX.csv',\n",
    "        'data/Wednesday-workingHours.pcap_ISCX.csv',\n",
    "        'data/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX_corrected.csv',\n",
    "        'data/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n",
    "        'data/Friday-WorkingHours-Morning.pcap_ISCX.csv',\n",
    "        'data/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',\n",
    "        'data/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData():\n",
    "    def __init__(self, source):\n",
    "        \n",
    "        self.all_labels = ['BENIGN', 'DoS Hulk', 'SSH-Patator', 'PortScan', 'DoS GoldenEye', \n",
    "                      'DDoS', 'Heartbleed', 'Web Attack  Brute Force', 'FTP-Patator', \n",
    "                      'Web Attack  XSS', 'DoS slowloris', 'Infiltration', 'Bot', \n",
    "                      'Web Attack  Sql Injection', 'DoS Slowhttptest']\n",
    "        \n",
    "        if '.pickle' in source:\n",
    "            sys.stdout.write('\\rfound previously created pickle file')\n",
    "            data      = pd.read_pickle(source)\n",
    "            self.Y    = data['label']\n",
    "            self.data = data.drop(['label'], axis=1)\n",
    "            sys.stdout.write('\\rpickle read completed')\n",
    "            \n",
    "        elif type(source) == str:\n",
    "            self.source = source\n",
    "            self.data     = None\n",
    "\n",
    "            self.read()\n",
    "            self.correct_column_names()\n",
    "            \n",
    "            # remove object columns\n",
    "            #self.data.drop(['flow_id', 'source_ip', 'destination_ip', 'timestamp'], axis=1, inplace=True)\n",
    "            self.data.drop(['flow_id', 'timestamp'], axis=1, inplace=True)\n",
    "            \n",
    "            self.change_labels_into_numbers()\n",
    "            self.to_numeric()\n",
    "            self.drop_nan()\n",
    "            \n",
    "            # remove some benign specific features. they all zero for non-benign classes\n",
    "            #benign_specific_features = [\"fwd urg flags\", \"cwe flag count\"]\n",
    "            #self.data.drop(benign_specific_features, axis=1, inplace=True)\n",
    "            #sys.stdout.write('\\r{}: some extra columns removed because of being benign-specific'.format(self.source))\n",
    "            \n",
    "            self.Y = self.data['label']\n",
    "            self.data.drop(['label'], axis=1, inplace=True)\n",
    "            \n",
    "            self.data.columns = list(map(lambda c: c.replace(' ', '_'), self.data.columns))\n",
    "        elif type(source) == list:\n",
    "            self.datasets = []\n",
    "            self.Ys       = []\n",
    "            counter       = 1\n",
    "            for filename in tqdm(source):\n",
    "                self.datasets.append(PrepareData(filename).data)\n",
    "                self.Ys.append(PrepareData(filename).Y)\n",
    "                counter += 1\n",
    "                sys.stdout.write('\\rprocess completed!')\n",
    "            self.merge_sets()\n",
    "            self.remove_allzeroes()\n",
    "            \n",
    "        ##########################################################################################\n",
    "        \n",
    "    def read(self):\n",
    "        \"\"\" \n",
    "            description     : reads and returns given CSV file contents\n",
    "        \"\"\"\n",
    "        sys.stdout.write('\\r{}: file is being read...'.format(self.source))\n",
    "        if self.source.split(\".\")[-1] == \"csv\":\n",
    "            self.data = pd.read_csv(self.source)\n",
    "        else:\n",
    "            self.data = pd.read_pickle(self.source)\n",
    "        ##########################################################################################\n",
    "\n",
    "    \n",
    "    def correct_column_names(self):\n",
    "        \"\"\"\n",
    "            description     : removes prefix spaces from columns names if any exists\n",
    "                              and makes all the names lower characters\n",
    "        \"\"\"\n",
    "        \n",
    "        columns_to_be_renamed = {}\n",
    "        for c in self.data.columns:\n",
    "            if c.startswith(' '):\n",
    "                columns_to_be_renamed[c] = c.strip().replace('  ', ' ')\n",
    "            \n",
    "\n",
    "        time.sleep(1)\n",
    "        self.data.rename(columns=columns_to_be_renamed, inplace=True)\n",
    "        # lower all column names\n",
    "        self.data.rename(columns={x: x.lower().replace(' ', '_') for x in self.data.columns}, inplace=True)\n",
    "        \n",
    "        sys.stdout.write('\\r{}: columns renamed'.format(self.source))\n",
    "        ##########################################################################################\n",
    "        \n",
    "    def change_labels_into_numbers(self):\n",
    "        \"\"\"\n",
    "            description     : turns labels into numbers starting from 0 to N-1\n",
    "        \"\"\"\n",
    "        # make labels counting from 0\n",
    "        for index, label in enumerate(self.all_labels):\n",
    "            self.data['label'] = np.where(self.data['label'] == label, index, self.data['label'])\n",
    "        sys.stdout.write('\\r{}: labels reindexed'.format(self.source))\n",
    "        ##########################################################################################\n",
    "\n",
    "    def to_numeric(self):\n",
    "        \"\"\"\n",
    "            description     : try to change str values into float. Else, make them NaN so we drop them later\n",
    "        \"\"\"\n",
    "        for i in self.data.columns:\n",
    "            if i not in ['destination_ip', 'source_ip']:\n",
    "                self.data[i] = pd.to_numeric(self.data[i], errors='coerce')\n",
    "        sys.stdout.write('\\r{}: values changed into float'.format(self.source))\n",
    "        ##########################################################################################\n",
    "        \n",
    "    def drop_nan(self):\n",
    "        \"\"\"\n",
    "            description     : converts -/+ inf values to NaN and removes the rows including any NaN values\n",
    "        \"\"\"\n",
    "        # change -inf and inf values to NaN in order to drop them\n",
    "        self.data.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        # drop rows including NaN values\n",
    "        before_row_count = self.data.shape[0]\n",
    "        self.data = self.data.dropna()\n",
    "        after_row_count = self.data.shape[0]\n",
    "        \n",
    "        sys.stdout.write(\"\\r{}: dropped {} rows\".format(self.source, before_row_count - after_row_count))\n",
    "        ##########################################################################################\n",
    "        \n",
    "    def change_nan(self, into='mean'):\n",
    "        \"\"\"\n",
    "            description     : change the NaN values into given options (min, max, mean, zero)\n",
    "        \"\"\"\n",
    "        if into == 'zero': \n",
    "            self.data[i].fillna(0, inplace=True)\n",
    "        else:\n",
    "            self.data[i].fillna(self.data[i].describe()[into], inplace=True)\n",
    "            \n",
    "        sys.stdout.write('\\r{}: NaN vlaues changed into {}'.format(self.source, into))\n",
    "        ##########################################################################################\n",
    "        \n",
    "    def drop_non_float_columns(self, columns):\n",
    "        \"\"\"\n",
    "            description     : drops given non-float(able) columns\n",
    "            parameters\n",
    "                columns     : list of string\n",
    "                              column names that are known to have non-floatable entities\n",
    "        \"\"\"\n",
    "        self.data.drop(columns, axis=1, inplace=True)\n",
    "        \n",
    "        sys.stdout.write('\\r{}: non-float(able) columns deleted'.format(self.source))\n",
    "        ##########################################################################################\n",
    "        \n",
    "    def cat_2_OHE(self, columns):\n",
    "        \"\"\"\n",
    "            description     : turn given categorical columns into one-hot numerical columns\n",
    "                columns     : list of string\n",
    "                              column names that are known to have categorial entities\n",
    "        \"\"\"\n",
    "        self.data = pd.get_dummies(self.data, columns=columns)\n",
    "        sys.stdout.write('\\r{}: categorical values changed into OHE'.format(self.source))\n",
    "        ##########################################################################################\n",
    "        \n",
    "    def merge_sets(self):\n",
    "        \"\"\"\n",
    "            description     : merges all subdatasets into one\n",
    "        \"\"\"\n",
    "        self.data     = pd.concat(self.datasets)\n",
    "        self.Y        = pd.concat(self.Ys)\n",
    "        # unassign these\n",
    "        self.datasets = None\n",
    "        self.Ys       = None\n",
    "        \n",
    "        sys.stdout.write('\\rall subsets merged')\n",
    "        ##########################################################################################\n",
    "\n",
    "    def remove_allzeroes(self):\n",
    "        \"\"\"\n",
    "            description     : removes columns that include only zero values\n",
    "        \"\"\"\n",
    "        \n",
    "        allzeroes = []\n",
    "        for i in self.data.columns:\n",
    "            if self.data[i].min() == 0 and self.data[i].max() == 0:\n",
    "                allzeroes.append(i)\n",
    "        self.data.drop(allzeroes, axis=1, inplace=True)\n",
    "        \n",
    "        sys.stdout.write('\\r {} columns including only zeroes removed: {}'.format(len(allzeroes), allzeroes))\n",
    "        ##########################################################################################\n",
    "\n",
    "    def cat_to_num(self, columns):\n",
    "        \"\"\"\n",
    "            description     : turns categorical columns into numeral ones\n",
    "        \"\"\"\n",
    "        \n",
    "        encoder = ce.BinaryEncoder(cols = columns)\n",
    "        encoder.fit(self.data, self.Y)\n",
    "        \n",
    "        self.data = encoder.transform(self.data)\n",
    "        \n",
    "        if (self.data.values[:,0].max() - self.data.values[:,0].min() == 0):\n",
    "            self.data.drop([self.data.columns[0]], axis=1, inplace=True)\n",
    "        \n",
    "        sys.stdout.write('\\rcategorical columns converted into numerical')\n",
    "        ##########################################################################################\n",
    "        \n",
    "    def normalize_dataset(self):\n",
    "        \"\"\"\n",
    "            description     : normalizes dataset with 0 mean and std 1\n",
    "        \"\"\"\n",
    "        values            = self.data.values\n",
    "        column_names      = self.data.columns\n",
    "        values_normalized = preprocessing.normalize(values)\n",
    "        self.data         = pd.DataFrame(values_normalized, columns=column_names)\n",
    "\n",
    "        sys.stdout.write('\\rdataset normalized')\n",
    "        ##########################################################################################\n",
    "        \n",
    "    def normalize(self):\n",
    "        \"\"\"\n",
    "            description     : normalizes sets with 0 mean and std 1\n",
    "        \"\"\"\n",
    "        self.X_train = preprocessing.normalize(self.X_train)\n",
    "        self.X_val = preprocessing.normalize(self.X_val)\n",
    "        self.X_test = preprocessing.normalize(self.X_test)\n",
    "        \n",
    "        sys.stdout.write('\\rtrain-test normalized')\n",
    "        ##########################################################################################\n",
    "        \n",
    "    def standardize_dataset(self):\n",
    "        \"\"\"\n",
    "            description     : standardizes dataset\n",
    "        \"\"\"\n",
    "        values              = self.data.values\n",
    "        column_names        = self.data.columns\n",
    "        scaler              = preprocessing.StandardScaler()\n",
    "        values_standardized = scaler.fit_transform(values)\n",
    "        self.data           = pd.DataFrame(values_standardized, columns=column_names)\n",
    "\n",
    "        sys.stdout.write('\\rdataset standardized')\n",
    "        ##########################################################################################\n",
    "        \n",
    "    def standardize(self):\n",
    "        \"\"\"\n",
    "            description     : standardizes train-test\n",
    "        \"\"\"\n",
    "\n",
    "        scaler              = preprocessing.StandardScaler()\n",
    "        self.X_train        = scaler.fit_transform(self.X_train)\n",
    "        \n",
    "        scaler              = preprocessing.StandardScaler()\n",
    "        self.X_val          = scaler.fit_transform(self.X_val)\n",
    "        \n",
    "        scaler              = preprocessing.StandardScaler()\n",
    "        self.X_test         = scaler.fit_transform(self.X_test)\n",
    "        \n",
    "        \n",
    "        sys.stdout.write('\\rtrain-test standardized')\n",
    "        ##########################################################################################\n",
    "        \n",
    "    def robustscale_dataset(self):\n",
    "        \"\"\"\n",
    "            description     : scales dataset to overcome some outliers\n",
    "        \"\"\"\n",
    "        \n",
    "        values              = self.data.values\n",
    "        column_names        = self.data.columns\n",
    "        scaler              = preprocessing.RobustScaler()\n",
    "        values_scaled       = scaler.fit_transform(values)\n",
    "        self.data           = pd.DataFrame(values_scaled, columns=column_names)\n",
    "\n",
    "        sys.stdout.write('\\rdataset robust scaled')\n",
    "        \n",
    "        \n",
    "    def robustscale(self):\n",
    "        \"\"\"\n",
    "            description     : scales train-test to overcome some outliers\n",
    "        \"\"\"\n",
    "\n",
    "        scaler1             = preprocessing.RobustScaler()\n",
    "        self.X_train        = scaler1.fit_transform(self.X_train)\n",
    "\n",
    "        scaler2             = preprocessing.RobustScaler()\n",
    "        self.X_val          = scaler2.fit_transform(self.X_val)\n",
    "        \n",
    "        scaler3             = preprocessing.RobustScaler()\n",
    "        self.X_test         = scaler3.fit_transform(self.X_test)\n",
    "\n",
    "        sys.stdout.write('\\rdataset robust scaled')\n",
    "        ##########################################################################################\n",
    "        \n",
    "    def scale_dataset(self, minval=0, maxval=1):\n",
    "        \"\"\"\n",
    "            description     : scales dataset between 0 - 1\n",
    "        \"\"\"\n",
    "        column_names   = self.data.columns\n",
    "        scaler         = preprocessing.MinMaxScaler(feature_range=(minval, maxval), copy=False)\n",
    "        scaler.fit(self.data)\n",
    "        values_scaled  = scaler.transform(self.data)\n",
    "        self.data      = pd.DataFrame(values_scaled, columns=column_names)\n",
    "\n",
    "        sys.stdout.write('\\rdataset scaled')\n",
    "        ##########################################################################################\n",
    "        \n",
    "    def scale(self):\n",
    "        \"\"\"\n",
    "            description     : scales train-test between 0 - 1\n",
    "        \"\"\"\n",
    "\n",
    "        scaler1             = preprocessing.MinMaxScaler(feature_range=(np.nextafter(0, 1), 1), copy=False)\n",
    "        self.X_train        = scaler1.fit_transform(self.X_train)\n",
    "        \n",
    "        scaler2             = preprocessing.MinMaxScaler(feature_range=(np.nextafter(0, 1), 1), copy=False)\n",
    "        self.X_val          = scaler2.fit_transform(self.X_val)\n",
    "        \n",
    "        scaler3             = preprocessing.MinMaxScaler(feature_range=(np.nextafter(0, 1), 1), copy=False)\n",
    "        self.X_test         = scaler3.fit_transform(self.X_test)\n",
    "        \n",
    "\n",
    "        sys.stdout.write('\\rtrain-test scaled')\n",
    "        ##########################################################################################\n",
    "        \n",
    "    def train_test_split(self, test_ratio=0.3, add_val=False):\n",
    "        column_names        = self.data.columns\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.data.values, self.Y, test_size=test_ratio, random_state=42)\n",
    "        if add_val:\n",
    "            self.X_train, self.X_val, self.y_train, self.y_val   = train_test_split(self.X_train, self.y_train, test_size=0.15, random_state=42)\n",
    "        \n",
    "        print(\"\"\"\n",
    "\n",
    "            train : {}\n",
    "            val   : {}\n",
    "            test  : {}\n",
    "        \"\"\".format(self.X_train.shape[0], self.X_val.shape[0] if add_val else 0, self.X_test.shape[0]))\n",
    "        \n",
    "        self.y_train = self.y_train.values\n",
    "        self.y_test  = self.y_test.values\n",
    "        \n",
    "        if add_val:\n",
    "            self.y_val   = self.y_val.values\n",
    "        \n",
    "    def save_dataset(self, outname=\"generated_dataset\", onlydataset=False, add_val=False):\n",
    "        if not onlydataset:\n",
    "            dataset = self.data.copy()\n",
    "            dataset['label'] = self.Y.values\n",
    "            dataset.to_pickle('{}.pickle'.format(outname))\n",
    "            sys.stdout.write('\\rdataset saved as {}.pickle'.format(outname))\n",
    "        \n",
    "        try:\n",
    "            if onlydataset:\n",
    "                h5f = h5py.File('sets/cicids_raw.h5', 'w')\n",
    "                h5f.create_dataset('X_train', data=self.X_train)\n",
    "                h5f.create_dataset('X_test',  data=self.X_test)\n",
    "                h5f.create_dataset('y_train', data=self.y_train)\n",
    "                h5f.create_dataset('y_test',  data=self.y_test)\n",
    "\n",
    "                if add_val:\n",
    "                    h5f.create_dataset('X_val',   data=self.X_val)\n",
    "                    h5f.create_dataset('y_val',   data=self.y_val)\n",
    "                h5f.close()\n",
    "        except:\n",
    "            pass\n",
    "        ##########################################################################################\n",
    "        \n",
    "#import pickle\n",
    "#pickle.dump(source, open('source_object.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Monday-WorkingHours.pcap_ISCX.csv: values changed into float"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:24<02:48, 24.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Tuesday-WorkingHours.pcap_ISCX.csv: values changed into float"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [00:43<02:15, 22.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Wednesday-workingHours.pcap_ISCX.csv: dropped 1297 rows"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [01:13<02:04, 24.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX_corrected.csv: values changed into float"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [01:21<01:19, 19.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv: values changed into float"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [01:35<00:54, 18.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Friday-WorkingHours-Morning.pcap_ISCX.csv: values changed into float"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [01:45<00:31, 15.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv: values changed into float"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [01:58<00:14, 14.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv: values changed into float"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [02:10<00:00, 16.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process completed!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8 columns including only zeroes removed: ['bwd_psh_flags', 'bwd_urg_flags', 'fwd_avg_bytes/bulk', 'fwd_avg_packets/bulk', 'fwd_avg_bulk_rate', 'bwd_avg_bytes/bulk', 'bwd_avg_packets/bulk', 'bwd_avg_bulk_rate']"
     ]
    }
   ],
   "source": [
    "source = PrepareData(dataset_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          192.168.10.5\n",
       "1          192.168.10.5\n",
       "2          192.168.10.5\n",
       "3          192.168.10.5\n",
       "4         192.168.10.14\n",
       "              ...      \n",
       "225740    192.168.10.15\n",
       "225741    192.168.10.15\n",
       "225742    192.168.10.15\n",
       "225743    192.168.10.15\n",
       "225744    192.168.10.15\n",
       "Name: destination_ip, Length: 2827876, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.data['destination_ip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical columns converted into numerical"
     ]
    }
   ],
   "source": [
    "source.cat_to_num(['source_port', 'destination_port', 'protocol', 'source_ip', 'destination_ip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.data.drop(['source_port', 'destination_port'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_port_0\n",
      "destination_ip_0\n",
      "destination_port_0\n",
      "protocol_0\n"
     ]
    }
   ],
   "source": [
    "for i in source.data.columns:\n",
    "    print(i) if source.data[i].max() == 0 else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove allzero new binary columns\n",
    "allzero_binaries = ['destination_port_0', 'protocol_0', 'destination_ip_0', 'source_port_0']\n",
    "source.data.drop(allzero_binaries, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2827876, 133)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## according to analysis, the following features are highly correlated: \n",
    "- fwd_header_length.1\n",
    "- fwd_packet_length_mean\n",
    "- syn_flag_count\n",
    "- total_length_of_fwd_packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop highly correlated features\n",
    "highly_correlated = [\"fwd_header_length.1\", \"fwd_packet_length_mean\", \"syn_flag_count\"]\n",
    "source.data.drop(highly_correlated, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset saved as raw_data.pickle"
     ]
    }
   ],
   "source": [
    "source.save_dataset(outname='raw_data', onlydataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_ip_1</th>\n",
       "      <th>source_ip_2</th>\n",
       "      <th>source_ip_3</th>\n",
       "      <th>source_ip_4</th>\n",
       "      <th>source_ip_5</th>\n",
       "      <th>source_ip_6</th>\n",
       "      <th>source_ip_7</th>\n",
       "      <th>source_ip_8</th>\n",
       "      <th>source_ip_9</th>\n",
       "      <th>source_ip_10</th>\n",
       "      <th>...</th>\n",
       "      <th>act_data_pkt_fwd</th>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <th>active_mean</th>\n",
       "      <th>active_std</th>\n",
       "      <th>active_max</th>\n",
       "      <th>active_min</th>\n",
       "      <th>idle_mean</th>\n",
       "      <th>idle_std</th>\n",
       "      <th>idle_max</th>\n",
       "      <th>idle_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.793084e-04</td>\n",
       "      <td>2.158228e-02</td>\n",
       "      <td>2.236884e-01</td>\n",
       "      <td>2.322828e-01</td>\n",
       "      <td>2.452848e-01</td>\n",
       "      <td>5.164901e-02</td>\n",
       "      <td>1.828659e-01</td>\n",
       "      <td>3.362506e-01</td>\n",
       "      <td>3.203623e-01</td>\n",
       "      <td>2.556679e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>5.423519e+00</td>\n",
       "      <td>-2.744494e+03</td>\n",
       "      <td>8.163400e+04</td>\n",
       "      <td>4.117582e+04</td>\n",
       "      <td>1.533378e+05</td>\n",
       "      <td>5.835492e+04</td>\n",
       "      <td>8.324468e+06</td>\n",
       "      <td>5.043548e+05</td>\n",
       "      <td>8.704568e+06</td>\n",
       "      <td>7.928061e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.605470e-02</td>\n",
       "      <td>1.453151e-01</td>\n",
       "      <td>4.167157e-01</td>\n",
       "      <td>4.222885e-01</td>\n",
       "      <td>4.302560e-01</td>\n",
       "      <td>2.213174e-01</td>\n",
       "      <td>3.865566e-01</td>\n",
       "      <td>4.724259e-01</td>\n",
       "      <td>4.666159e-01</td>\n",
       "      <td>4.362360e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>6.367482e+02</td>\n",
       "      <td>1.085539e+06</td>\n",
       "      <td>6.489234e+05</td>\n",
       "      <td>3.935787e+05</td>\n",
       "      <td>1.026333e+06</td>\n",
       "      <td>5.773818e+05</td>\n",
       "      <td>2.364057e+07</td>\n",
       "      <td>4.605289e+06</td>\n",
       "      <td>2.437766e+07</td>\n",
       "      <td>2.337390e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-5.368707e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.400000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>3.200000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.135570e+05</td>\n",
       "      <td>1.380000e+02</td>\n",
       "      <td>1.100000e+08</td>\n",
       "      <td>7.420000e+07</td>\n",
       "      <td>1.100000e+08</td>\n",
       "      <td>1.100000e+08</td>\n",
       "      <td>1.200000e+08</td>\n",
       "      <td>7.690000e+07</td>\n",
       "      <td>1.200000e+08</td>\n",
       "      <td>1.200000e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        source_ip_1   source_ip_2   source_ip_3   source_ip_4   source_ip_5  \\\n",
       "count  2.827876e+06  2.827876e+06  2.827876e+06  2.827876e+06  2.827876e+06   \n",
       "mean   6.793084e-04  2.158228e-02  2.236884e-01  2.322828e-01  2.452848e-01   \n",
       "std    2.605470e-02  1.453151e-01  4.167157e-01  4.222885e-01  4.302560e-01   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "\n",
       "        source_ip_6   source_ip_7   source_ip_8   source_ip_9  source_ip_10  \\\n",
       "count  2.827876e+06  2.827876e+06  2.827876e+06  2.827876e+06  2.827876e+06   \n",
       "mean   5.164901e-02  1.828659e-01  3.362506e-01  3.203623e-01  2.556679e-01   \n",
       "std    2.213174e-01  3.865566e-01  4.724259e-01  4.666159e-01  4.362360e-01   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "\n",
       "       ...  act_data_pkt_fwd  min_seg_size_forward   active_mean  \\\n",
       "count  ...      2.827876e+06          2.827876e+06  2.827876e+06   \n",
       "mean   ...      5.423519e+00         -2.744494e+03  8.163400e+04   \n",
       "std    ...      6.367482e+02          1.085539e+06  6.489234e+05   \n",
       "min    ...      0.000000e+00         -5.368707e+08  0.000000e+00   \n",
       "25%    ...      0.000000e+00          2.000000e+01  0.000000e+00   \n",
       "50%    ...      1.000000e+00          2.400000e+01  0.000000e+00   \n",
       "75%    ...      2.000000e+00          3.200000e+01  0.000000e+00   \n",
       "max    ...      2.135570e+05          1.380000e+02  1.100000e+08   \n",
       "\n",
       "         active_std    active_max    active_min     idle_mean      idle_std  \\\n",
       "count  2.827876e+06  2.827876e+06  2.827876e+06  2.827876e+06  2.827876e+06   \n",
       "mean   4.117582e+04  1.533378e+05  5.835492e+04  8.324468e+06  5.043548e+05   \n",
       "std    3.935787e+05  1.026333e+06  5.773818e+05  2.364057e+07  4.605289e+06   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "max    7.420000e+07  1.100000e+08  1.100000e+08  1.200000e+08  7.690000e+07   \n",
       "\n",
       "           idle_max      idle_min  \n",
       "count  2.827876e+06  2.827876e+06  \n",
       "mean   8.704568e+06  7.928061e+06  \n",
       "std    2.437766e+07  2.337390e+07  \n",
       "min    0.000000e+00  0.000000e+00  \n",
       "25%    0.000000e+00  0.000000e+00  \n",
       "50%    0.000000e+00  0.000000e+00  \n",
       "75%    0.000000e+00  0.000000e+00  \n",
       "max    1.200000e+08  1.200000e+08  \n",
       "\n",
       "[8 rows x 130 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start from here for raw data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle read completed"
     ]
    }
   ],
   "source": [
    "source = PrepareData('raw_data.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_ip_1</th>\n",
       "      <th>source_ip_2</th>\n",
       "      <th>source_ip_3</th>\n",
       "      <th>source_ip_4</th>\n",
       "      <th>source_ip_5</th>\n",
       "      <th>source_ip_6</th>\n",
       "      <th>source_ip_7</th>\n",
       "      <th>source_ip_8</th>\n",
       "      <th>source_ip_9</th>\n",
       "      <th>source_ip_10</th>\n",
       "      <th>...</th>\n",
       "      <th>act_data_pkt_fwd</th>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <th>active_mean</th>\n",
       "      <th>active_std</th>\n",
       "      <th>active_max</th>\n",
       "      <th>active_min</th>\n",
       "      <th>idle_mean</th>\n",
       "      <th>idle_std</th>\n",
       "      <th>idle_max</th>\n",
       "      <th>idle_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "      <td>2.827876e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.793084e-04</td>\n",
       "      <td>2.158228e-02</td>\n",
       "      <td>2.236884e-01</td>\n",
       "      <td>2.322828e-01</td>\n",
       "      <td>2.452848e-01</td>\n",
       "      <td>5.164901e-02</td>\n",
       "      <td>1.828659e-01</td>\n",
       "      <td>3.362506e-01</td>\n",
       "      <td>3.203623e-01</td>\n",
       "      <td>2.556679e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>5.423519e+00</td>\n",
       "      <td>-2.744494e+03</td>\n",
       "      <td>8.163400e+04</td>\n",
       "      <td>4.117582e+04</td>\n",
       "      <td>1.533378e+05</td>\n",
       "      <td>5.835492e+04</td>\n",
       "      <td>8.324468e+06</td>\n",
       "      <td>5.043548e+05</td>\n",
       "      <td>8.704568e+06</td>\n",
       "      <td>7.928061e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.605470e-02</td>\n",
       "      <td>1.453151e-01</td>\n",
       "      <td>4.167157e-01</td>\n",
       "      <td>4.222885e-01</td>\n",
       "      <td>4.302560e-01</td>\n",
       "      <td>2.213174e-01</td>\n",
       "      <td>3.865566e-01</td>\n",
       "      <td>4.724259e-01</td>\n",
       "      <td>4.666159e-01</td>\n",
       "      <td>4.362360e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>6.367482e+02</td>\n",
       "      <td>1.085539e+06</td>\n",
       "      <td>6.489234e+05</td>\n",
       "      <td>3.935787e+05</td>\n",
       "      <td>1.026333e+06</td>\n",
       "      <td>5.773818e+05</td>\n",
       "      <td>2.364057e+07</td>\n",
       "      <td>4.605289e+06</td>\n",
       "      <td>2.437766e+07</td>\n",
       "      <td>2.337390e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-5.368707e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.400000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>3.200000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.135570e+05</td>\n",
       "      <td>1.380000e+02</td>\n",
       "      <td>1.100000e+08</td>\n",
       "      <td>7.420000e+07</td>\n",
       "      <td>1.100000e+08</td>\n",
       "      <td>1.100000e+08</td>\n",
       "      <td>1.200000e+08</td>\n",
       "      <td>7.690000e+07</td>\n",
       "      <td>1.200000e+08</td>\n",
       "      <td>1.200000e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        source_ip_1   source_ip_2   source_ip_3   source_ip_4   source_ip_5  \\\n",
       "count  2.827876e+06  2.827876e+06  2.827876e+06  2.827876e+06  2.827876e+06   \n",
       "mean   6.793084e-04  2.158228e-02  2.236884e-01  2.322828e-01  2.452848e-01   \n",
       "std    2.605470e-02  1.453151e-01  4.167157e-01  4.222885e-01  4.302560e-01   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "\n",
       "        source_ip_6   source_ip_7   source_ip_8   source_ip_9  source_ip_10  \\\n",
       "count  2.827876e+06  2.827876e+06  2.827876e+06  2.827876e+06  2.827876e+06   \n",
       "mean   5.164901e-02  1.828659e-01  3.362506e-01  3.203623e-01  2.556679e-01   \n",
       "std    2.213174e-01  3.865566e-01  4.724259e-01  4.666159e-01  4.362360e-01   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "\n",
       "       ...  act_data_pkt_fwd  min_seg_size_forward   active_mean  \\\n",
       "count  ...      2.827876e+06          2.827876e+06  2.827876e+06   \n",
       "mean   ...      5.423519e+00         -2.744494e+03  8.163400e+04   \n",
       "std    ...      6.367482e+02          1.085539e+06  6.489234e+05   \n",
       "min    ...      0.000000e+00         -5.368707e+08  0.000000e+00   \n",
       "25%    ...      0.000000e+00          2.000000e+01  0.000000e+00   \n",
       "50%    ...      1.000000e+00          2.400000e+01  0.000000e+00   \n",
       "75%    ...      2.000000e+00          3.200000e+01  0.000000e+00   \n",
       "max    ...      2.135570e+05          1.380000e+02  1.100000e+08   \n",
       "\n",
       "         active_std    active_max    active_min     idle_mean      idle_std  \\\n",
       "count  2.827876e+06  2.827876e+06  2.827876e+06  2.827876e+06  2.827876e+06   \n",
       "mean   4.117582e+04  1.533378e+05  5.835492e+04  8.324468e+06  5.043548e+05   \n",
       "std    3.935787e+05  1.026333e+06  5.773818e+05  2.364057e+07  4.605289e+06   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "max    7.420000e+07  1.100000e+08  1.100000e+08  1.200000e+08  7.690000e+07   \n",
       "\n",
       "           idle_max      idle_min  \n",
       "count  2.827876e+06  2.827876e+06  \n",
       "mean   8.704568e+06  7.928061e+06  \n",
       "std    2.437766e+07  2.337390e+07  \n",
       "min    0.000000e+00  0.000000e+00  \n",
       "25%    0.000000e+00  0.000000e+00  \n",
       "50%    0.000000e+00  0.000000e+00  \n",
       "75%    0.000000e+00  0.000000e+00  \n",
       "max    1.200000e+08  1.200000e+08  \n",
       "\n",
       "[8 rows x 130 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset scaled"
     ]
    }
   ],
   "source": [
    "source.scale_dataset(minval=0, maxval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.save_dataset(outname='raw_data', onlydataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
